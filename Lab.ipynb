{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab - Training `and` and `xor` Feedforward NNs in `torch`\n",
    "This notebook serves as the starter code and lab description covering **Chapter 21 - Deep Learning (Part 1)** from the book *Artificial Intelligence: A Modern Approach.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cpu')\n",
    "# if you are using a gpu or you want your code be flexibly running over both CPU and GPU use the following line instead:\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERVIEW\n",
    "In the lecture, we talked about the simple feedforward networks and the way they work. In this lab, we learn to use `torch` library to create and train such networks.\n",
    "\n",
    "## First, training `and` network\n",
    "The first network that we create is a simple network to perform binary `and` operation on two binary inputs and calculate the result. For this purpose, lets first create the input/output data. In `torch`, the networks work with an object type called `Tensor`. Simply put, tensors are multi-dimensional matrices (e.g. a 1-d tensor is an actual vector, 2-d tensors are also equivalent to want you have seen as a matrix, higher than 2-d tensors are also valid and can be used to represent different dimensions of data). Read [here](https://pytorch.org/docs/stable/tensors.html) and familiarize yourself with different tensor types.\n",
    "\n",
    "In our example, we create input/output pairs as `torch.FloatTensor` as most of the loss calculation objects in torch work with floating point typed tensors. \n",
    "\n",
    "**Note(1):** you remember that in the lecture we talked about batching the input data while training. `torch` library lets you batch up many input records in one `torch.FloatTensor` object.  However, as we are not going to use multi-record batches (for the sake of simplicity), you see the records receiving a 2-dimentional list containing only one input record (e.g. torch.FloatTensor(**\\[\\[0, 0\\]\\]**)).\n",
    "\n",
    "**Note(2):** each index of input object (`data_x`) pairs up with the item with the same index in output object (`data_y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are expected binary input/output pairs defined using torch tensors\n",
    "data_x = [\n",
    "    torch.FloatTensor([[0, 0]]), \n",
    "    torch.FloatTensor([[0, 1]]), \n",
    "    torch.FloatTensor([[1, 0]]), \n",
    "    torch.FloatTensor([[1, 1]])\n",
    "]\n",
    "data_y = [\n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([1])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the network input/outputs ready, we need to create the feedforward network model. Our model, however, does not have to be really complicated. It wil be just an instance of `torch.nn.Linear` (the implementation of the simple feedforward layer in `torch` library. Read more about it [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).\n",
    "\n",
    "For the next step, create an instance of `torch.nn.Linear` with the input dimension size of `2` and output dimension size of `1`. For more help, you can use the documentation of the `torch.nn.Linear` class, mentioned earlier.\n",
    "\n",
    "**Note: don't forget to call `.to(device)` on any model that you create to support underlying hardware changes from GPU to CPU and the reverse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create the `model` object here\n",
    "model = torch.nn.Linear(2, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing we need is an instance of a loss calculation object which can help the training process. [Here](https://pytorch.org/docs/stable/nn.html#loss-functions) you can find a complete list of all the implemented loss functions in `torch` library. For this section, we focus on mean squeared error loss function [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n",
    "\n",
    "Read through the documentation of `torch.nn.MSELoss` and create an instance of it with the parameter (`reduction='sum'`) which guides the network to augment the loss values for each instance by summing them up (as opposed to averaging them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create and instance of `MSELoss` and call it loss_fn\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a function `train` which\n",
    "1. Receives an input/output pair (e.g. one `(x,y)` pair from the pairs we stored in `data_x` and `data_y`).\n",
    " - It should also receive the model instance as well as the `learning_rate` parameter.\n",
    "2. Calculates the model prediction on `x`.\n",
    "3. Calculates the loss using the model prediction and `y`.\n",
    "4. Runs `model.zero_grad()` to reset the gradient variables of the computation graph in the network.\n",
    "5. Performs `backward()` on the loss.\n",
    "6. Updates each of model parameters (e.g. `param in model.parameters()`) with the value of `learning_rate * param.grad`.\n",
    " - Make sure this step is done under `with torch.no_grad()` block.\n",
    "7. Returns the value of loss at the end.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model(data_x)\n",
    "# out = loss_fn(data_x,data_y)\n",
    "# model.zero_rad()\n",
    "# out.backward()\n",
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO implement `train` function here\n",
    "\n",
    "def train(x,y, model, learning_rate):\n",
    "    model.zero_grad()\n",
    "    Y_hat = model(x)\n",
    "    loss = loss_fn(Y_hat,y)\n",
    "    \n",
    "    loss.backward()\n",
    "#     with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param = param.data+ learning_rate* param.grad\n",
    "            \n",
    "    return loss.item()    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last training step would be to use the function we just implemented to train the feedforward `model` which we created earlier. \n",
    "- Set the value of `learning_rate` to `1`.\n",
    "- Iterate over the training data (pairs in `data_x` and `data_y`) for about 500 epochs (this should be done really quickly so don't worry). \n",
    "  - After each epoch if the loss of the last instance (or the average of the losses of all the instances to be more accurate!) is less than `0.01`, make sure you break the outer loop (which is performing training epochs). This is called early stopping. \n",
    "  - Print out the value of loss (you can comment this line in your submission).\n",
    "  - After each 50 epochs, half the `learning_rate` (this is called learning rate decay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO perform the training loop in here\n",
    "learning_rate= 1\n",
    "epoch = 500\n",
    "loss = 0\n",
    "loss_avg = 0\n",
    "for i in range(epoch):\n",
    "    for x,y in zip(data_x, data_y):\n",
    "#             print(x,y)\n",
    "        loss = loss + train(x,y,model,learning_rate)\n",
    "            \n",
    "    loss_avg = loss_avg + (loss/len(data_x))\n",
    "    if loss_avg < 0.1:\n",
    "        break\n",
    "    if epoch == 50:\n",
    "        learning_rate = learning_rate/2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will perform an evaluation on the model you just trained (I expect you get 4 `Correctly predicted` messages, if you didn't re-run the training cell and then re-run the test cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0397,  0.0941]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2987], requires_grad=True)\n",
      "[0. 0.] \tPrediction: False\tCorrectly predicted.\n",
      "[0. 1.] \tPrediction: False\tCorrectly predicted.\n",
      "[1. 0.] \tPrediction: False\tCorrectly predicted.\n",
      "[1. 1.] \tPrediction: False\tIncorrectly predicted.\n"
     ]
    }
   ],
   "source": [
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "for x, y in zip(data_x, data_y):\n",
    "    y_pred = model(x) > 0.5\n",
    "    print(x.numpy()[0], \"\\tPrediction: {}\\t{} predicted.\".format(y_pred.item(), \"Correctly\" if y.item() == y_pred.item() else \"Incorrectly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, training `xor` network\n",
    "Now that we have some hands on experience try reusing what you just implemented on the following train set representing binary `xor` operation, and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [\n",
    "    torch.FloatTensor([[0, 0]]), \n",
    "    torch.FloatTensor([[0, 1]]), \n",
    "    torch.FloatTensor([[1, 0]]), \n",
    "    torch.FloatTensor([[1, 1]])\n",
    "]\n",
    "data_y = [\n",
    "    torch.FloatTensor([1]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([0]), \n",
    "    torch.FloatTensor([1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO re-create the model object and reuse the training loop and report the test results for it here\n",
    "model_xor = torch.nn.Linear(2, 1)\n",
    "# TODO perform the training loop in here\n",
    "learning_rate= 1\n",
    "epoch = 500\n",
    "loss = 0\n",
    "loss_avg = 0\n",
    "for i in range(epoch):\n",
    "    for x,y in zip(data_x, data_y):\n",
    "#             print(x,y)\n",
    "        loss = loss + train(x,y,model_xor,learning_rate)\n",
    "            \n",
    "    loss_avg = loss_avg + (loss/len(data_x))\n",
    "    if loss_avg < 0.1:\n",
    "        break\n",
    "    if epoch == 50:\n",
    "        learning_rate = learning_rate/2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training `xor` network with multi-layer perceptron (MLP) module\n",
    "Now lets's try a more complex model for our `xor` dataset. The model we intend to use will have two fully-connected (feedforward; `torch.nn.Linear`) layers put together using [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) container. Look at [here](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to see how Sequential container works. For a better performance, have the `Sequential` instance pass the results of the first fully-connected layer through a `torch.nn.Tanh` non-linearity before passing it through the second fully-connected layer. \n",
    "\n",
    "Using this architecture, your model will first map the input of size `2` to a hidden value of size `4` (we are setting these values) and then maps the hidden vector to the output of size `1`. In the next cell, implement this model and use the `xor` data and the `train` function and the created `loss_fn` instance to train the new model.\n",
    "\n",
    "**Note(1): \\[again\\] don't forget to call `.to(device)` on any model that you create to support underlying hardware changes from GPU to CPU and the reverse.**\n",
    "\n",
    "**Note(2): since this new model has more parameters, set the initial value of `learning_rate` to `0.25` instead of `1`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create the MLP module here\n",
    "learning_rate= 0.25\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(2, 1),\n",
    "          torch.nn.Tanh() ,\n",
    "          torch.nn.Linear(2, 1)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will perform an evaluation on the model you just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2211, -0.2304]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4005], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3336,  0.6473]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4531], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 2x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-1972a2cb18d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\tPrediction: {}\\t{} predicted.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Correctly\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"Incorrectly\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sehaj\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sehaj\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sehaj\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sehaj\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sehaj\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 2x1)"
     ]
    }
   ],
   "source": [
    "print(model[0].weight)\n",
    "print(model[0].bias)\n",
    "print(model[2].weight)\n",
    "print(model[2].bias)\n",
    "\n",
    "for x, y in zip(data_x, data_y):\n",
    "    y_pred = model(x) > 0.5\n",
    "    print(x.numpy()[0], \"\\tPrediction: {}\\t{} predicted.\".format(y_pred.item(), \"Correctly\" if y.item() == y_pred.item() else \"Incorrectly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the prediction results of the MLP model with the single layer perceptron (the one we first trained on the `xor` data) in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "You definitely need to learn `torch` in great detail. [Here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) is where you can get started. Please talk to me if you had any problem understanding the examples provided in this link."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
